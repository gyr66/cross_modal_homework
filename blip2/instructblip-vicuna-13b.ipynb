{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration, InstructBlipConfig\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_POINT = \"Salesforce/instructblip-vicuna-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yirguo/data/conda_env/cross_modal_homework/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = InstructBlipConfig.from_pretrained(CHECK_POINT)\n",
    "with init_empty_weights():\n",
    "    model = InstructBlipForConditionalGeneration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    }
   ],
   "source": [
    "model.tie_weights()\n",
    "device_map = infer_auto_device_map(model, no_split_module_classes=[\"LlamaDecoderLayer\", \"VisionTransformer\"], dtype=\"float32\", max_memory={0: \"10GiB\", 1: \"10GiB\", 2: \"10Gib\", 3: \"10Gib\", 4: \"10Gib\", 5: \"10Gib\", 6: \"10Gib\", 7: \"10Gib\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('query_tokens', 0),\n",
       "             ('vision_model', 0),\n",
       "             ('qformer', 0),\n",
       "             ('language_projection', 0),\n",
       "             ('language_model.model.embed_tokens', 0),\n",
       "             ('language_model.model.layers.0', 0),\n",
       "             ('language_model.model.layers.1', 0),\n",
       "             ('language_model.model.layers.2', 0),\n",
       "             ('language_model.model.layers.3', 1),\n",
       "             ('language_model.model.layers.4', 1),\n",
       "             ('language_model.model.layers.5', 1),\n",
       "             ('language_model.model.layers.6', 1),\n",
       "             ('language_model.model.layers.7', 1),\n",
       "             ('language_model.model.layers.8', 1),\n",
       "             ('language_model.model.layers.9', 1),\n",
       "             ('language_model.model.layers.10', 1),\n",
       "             ('language_model.model.layers.11', 2),\n",
       "             ('language_model.model.layers.12', 2),\n",
       "             ('language_model.model.layers.13', 2),\n",
       "             ('language_model.model.layers.14', 2),\n",
       "             ('language_model.model.layers.15', 2),\n",
       "             ('language_model.model.layers.16', 2),\n",
       "             ('language_model.model.layers.17', 2),\n",
       "             ('language_model.model.layers.18', 2),\n",
       "             ('language_model.model.layers.19', 3),\n",
       "             ('language_model.model.layers.20', 3),\n",
       "             ('language_model.model.layers.21', 3),\n",
       "             ('language_model.model.layers.22', 3),\n",
       "             ('language_model.model.layers.23', 3),\n",
       "             ('language_model.model.layers.24', 3),\n",
       "             ('language_model.model.layers.25', 3),\n",
       "             ('language_model.model.layers.26', 3),\n",
       "             ('language_model.model.layers.27', 4),\n",
       "             ('language_model.model.layers.28', 4),\n",
       "             ('language_model.model.layers.29', 4),\n",
       "             ('language_model.model.layers.30', 4),\n",
       "             ('language_model.model.layers.31', 4),\n",
       "             ('language_model.model.layers.32', 4),\n",
       "             ('language_model.model.layers.33', 4),\n",
       "             ('language_model.model.layers.34', 4),\n",
       "             ('language_model.model.layers.35', 5),\n",
       "             ('language_model.model.layers.36', 5),\n",
       "             ('language_model.model.layers.37', 5),\n",
       "             ('language_model.model.layers.38', 5),\n",
       "             ('language_model.model.layers.39', 5),\n",
       "             ('language_model.model.norm', 5),\n",
       "             ('language_model.lm_head', 5)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map['language_model.lm_head'] = device_map['language_model.model.embed_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('query_tokens', 0),\n",
       "             ('vision_model', 0),\n",
       "             ('qformer', 0),\n",
       "             ('language_projection', 0),\n",
       "             ('language_model.model.embed_tokens', 0),\n",
       "             ('language_model.model.layers.0', 0),\n",
       "             ('language_model.model.layers.1', 0),\n",
       "             ('language_model.model.layers.2', 0),\n",
       "             ('language_model.model.layers.3', 1),\n",
       "             ('language_model.model.layers.4', 1),\n",
       "             ('language_model.model.layers.5', 1),\n",
       "             ('language_model.model.layers.6', 1),\n",
       "             ('language_model.model.layers.7', 1),\n",
       "             ('language_model.model.layers.8', 1),\n",
       "             ('language_model.model.layers.9', 1),\n",
       "             ('language_model.model.layers.10', 1),\n",
       "             ('language_model.model.layers.11', 2),\n",
       "             ('language_model.model.layers.12', 2),\n",
       "             ('language_model.model.layers.13', 2),\n",
       "             ('language_model.model.layers.14', 2),\n",
       "             ('language_model.model.layers.15', 2),\n",
       "             ('language_model.model.layers.16', 2),\n",
       "             ('language_model.model.layers.17', 2),\n",
       "             ('language_model.model.layers.18', 2),\n",
       "             ('language_model.model.layers.19', 3),\n",
       "             ('language_model.model.layers.20', 3),\n",
       "             ('language_model.model.layers.21', 3),\n",
       "             ('language_model.model.layers.22', 3),\n",
       "             ('language_model.model.layers.23', 3),\n",
       "             ('language_model.model.layers.24', 3),\n",
       "             ('language_model.model.layers.25', 3),\n",
       "             ('language_model.model.layers.26', 3),\n",
       "             ('language_model.model.layers.27', 4),\n",
       "             ('language_model.model.layers.28', 4),\n",
       "             ('language_model.model.layers.29', 4),\n",
       "             ('language_model.model.layers.30', 4),\n",
       "             ('language_model.model.layers.31', 4),\n",
       "             ('language_model.model.layers.32', 4),\n",
       "             ('language_model.model.layers.33', 4),\n",
       "             ('language_model.model.layers.34', 4),\n",
       "             ('language_model.model.layers.35', 5),\n",
       "             ('language_model.model.layers.36', 5),\n",
       "             ('language_model.model.layers.37', 5),\n",
       "             ('language_model.model.layers.38', 5),\n",
       "             ('language_model.model.layers.39', 5),\n",
       "             ('language_model.model.norm', 5),\n",
       "             ('language_model.lm_head', 0)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc200f4623104cd8bb981d38925b1b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yirguo/data/conda_env/cross_modal_homework/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n",
      "/home/yirguo/data/conda_env/cross_modal_homework/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the image, there is a large brick building with a clock tower on top of it. The building is situated in the middle of a city street, surrounded by trees and other buildings. There is also a car parked on the side of the road near the building.\n"
     ]
    }
   ],
   "source": [
    "model = InstructBlipForConditionalGeneration.from_pretrained(CHECK_POINT, device_map=device_map)\n",
    "processor = InstructBlipProcessor.from_pretrained(CHECK_POINT)\n",
    "\n",
    "url = \"https://gker-love.oss-cn-beijing.aliyuncs.com/Naive/messages/6e6c01ed-29bb-447d-8790-4f068d0b6e8a/da6a1872-5d75-478d-a5ac-8e5e24864df4.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "prompt = \"What do you see in the image?\"\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        max_length=256,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.5,\n",
    "        length_penalty=1.0,\n",
    "        temperature=1,\n",
    ")\n",
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross_modal_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
